{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Neural Networks: Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations\n",
    "\n",
    "### Non-linear Hypotheses\n",
    "\n",
    "Say we had a classification task with two features and the 'line' separating the positive class from the negative was a fairly complicated shape, such that to represent this line we needed to include the quadratic and 3rd degree polynomial terms in the hypothesis.\n",
    "\n",
    "Our hypothesis would look something like:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2 + \\theta_5x_1x_2 + \\theta_6x_1^3 + \\theta_7x_2^3 + \\theta_8x_1^2x_2 + \\dots )$$\n",
    "\n",
    "Even with just two original features and including up to only the 3rd degree polynomial, the number of overall features has increased enormously.\n",
    "\n",
    "A lot of problems do require a lot of input features. For example, in computer vision if you use pixels as input features even if you restricted yourself to just grayscale 50x50 images you would have 2500 original input features. If you wanted to include the quadratic terms that gives you 3 million features, already too large to be reasonable.\n",
    "\n",
    "### Neurons and the Brain\n",
    "\n",
    "Neural networks are biologically inspired learning algorithms, i.e. algorithms that mimic the brain. Recent resurgence as it was only recently that computers became fast enough to run large scale neural networks.\n",
    "\n",
    "\"One learning algorithm\" hypothesis:  \n",
    "The brain uses a single learning algorithm. Evidence: successful neuro-rewiring experiments, e.g. rewiring auditory cortex so that it's being fed signals from the optic nerve; auditory cortex then learns to see. Idea is that if a single piece of brain tissue that can process sight / sound / touch then maybe there's one learning algorithm that can process sight / sound / touch. If we can figure out what that learning algorithm is and it we'll be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Model Representation I\n",
    "\n",
    "Neurons have dendrites, which can be thought of as \"input wires\", and an axon, which can be thought of as an \"output wire\". When a neuron receives a bunch of electrical pulses via its dendrites its axon will either fire an electrical pulse (to be received as an input to another neuron's dendrites) or not.\n",
    "\n",
    "We represent a single neuron as a \"logistic unit\":\n",
    "\n",
    "![logistic unit; credit: Andrew Ng Coursera](https://beths3test.s3.amazonaws.com/machine-learning-notes/neuron.png)\n",
    "\n",
    "In this diagram:\n",
    "\n",
    "$$ x = \\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\theta = \\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\theta_3 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "You can draw the $x_0$ node (the \"bias unit/node\"), but since it's always 0 it often gets left out.  \n",
    "As we're using a sigmoid function, this neuron is said to have a sigmoid (logistic) activation function.\n",
    "\n",
    "A neural network is a load of these logistic units connected together:\n",
    "\n",
    "![neural network; credit: Andrew Ng Coursera](https://beths3test.s3.amazonaws.com/machine-learning-notes/neural-network.png)\n",
    "\n",
    "In this diagram, layer 1 is the input layer, layer 3 is the output layer and layer 2 the hidden layer. Again, for each layer (except the output layer) there is a bias unit that isn't always drawn.\n",
    "\n",
    "$a^{(j)}_i$ is the \"activation\" of unit $i$ in layer $j$.  \n",
    "$\\Theta^{(j)}$ is the matrix of weights controlling function mapping from layer $j$ to layer $j + 1$.\n",
    "\n",
    "$h_\\Theta(x)$ in this network gets computed the following way:\n",
    "\n",
    "First, the activations of the nodes in the hidden layer are calculated:\n",
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2) + \\Theta_{13}^{(1)}x_3) \\\\\n",
    "a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2) + \\Theta_{23}^{(1)}x_3) \\\\\n",
    "a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2) + \\Theta_{33}^{(1)}x_3)$$\n",
    "\n",
    "Then, these results are used to calculate the output:\n",
    "\n",
    "$$ h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)}) + \\Theta_{13}^{(2)}a_3^{(2)}) $$\n",
    "\n",
    "If a network has $s_j$ units in layer $j$, $s_{j + 1}$ units in layer $j + 1$, then $\\Theta^{(j)}$ will be of dimension $s_{j + 1} \\times (s_j + 1)$.\n",
    "\n",
    "In our diagram, there are 3 units in layer 1 and 3 units in layer 2, and so $\\Theta^{(1)}$ is a $(3\\times4)$ matrix. There are 3 units in layer 2 and 1 in layer 3, so $\\Theta^{(2)}$ is a $(1\\times4)$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation II\n",
    "\n",
    "Let's try and work out a vectorized implementation of the neural network described above.\n",
    "\n",
    "Before we described our network as:\n",
    "\n",
    "$$ a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2) + \\Theta_{13}^{(1)}x_3) \\\\\n",
    "a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2) + \\Theta_{23}^{(1)}x_3) \\\\\n",
    "a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2) + \\Theta_{33}^{(1)}x_3) \\\\\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)}) + \\Theta_{13}^{(2)}a_3^{(2)}) $$\n",
    "\n",
    "Let's write the parameters passed to $g(z)$ like this:\n",
    "\n",
    "$$ z_1^{(2)} = \\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2) + \\Theta_{13}^{(1)}x_3 \\\\\n",
    "z_2^{(2)} = \\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2) + \\Theta_{23}^{(1)}x_3 \\\\\n",
    "z_3^{(2)} = \\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2) + \\Theta_{33}^{(1)}x_3 $$\n",
    "\n",
    "So now we can say that:\n",
    "\n",
    "$$ a_1^{(2)} = g(z_1^{(2)}) \\\\\n",
    "a_2^{(2)} = g(z_2^{(2)}) \\\\\n",
    "a_3^{(2)} = g(z_3^{(2)})$$\n",
    "\n",
    "Let's define the vector $z^{(2)}$ as:\n",
    "\n",
    "$$ z^{(2)} = \\begin{bmatrix}\n",
    "z_1^{(2)} \\\\\n",
    "z_2^{(2)} \\\\\n",
    "z_3^{(2)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you look at how the element's of $z^{(2)}$ are defined you can see that $z^{(2)} = \\Theta^{(1)}x$. So we can calculate the vector $a^{(2)}$ (the second, hidden layer) in just two steps:\n",
    "\n",
    "$$ z^{(2)} = \\Theta^{(1)}x \\\\\n",
    "   a^{(2)} = g(z^{(2)})$$\n",
    "   \n",
    "Now we need to add the bias unit $a_0^{(2)} (= 1)$ to the hidden layer so that:\n",
    "\n",
    "$$ a^{(2)} = \\begin{bmatrix}\n",
    "a_0^{(2)} \\\\\n",
    "a_1^{(2)} \\\\\n",
    "a_2^{(2)} \\\\\n",
    "a_3^{(2)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And now we can write:\n",
    "\n",
    "$$z^{(3)} = \\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)}) + \\Theta_{13}^{(2)}a_3^{(2)}$$\n",
    "\n",
    "As:\n",
    "\n",
    "$$ z^{(3)} = \\Theta^{(2)}a^{(2)} $$\n",
    "\n",
    "So that the final output of the network is:\n",
    "\n",
    "$$ h_\\Theta(x) = a^{(3)} = g(z^{(3)}) $$\n",
    "\n",
    "This process is known as *forward propagation* because each layer is computed and the result used to compute the next layer until we arrive at the output.\n",
    "\n",
    "This is cool because instead of being constrained to use the raw features $x$, this network gets to learn the features $a^{(2)}$, and use those as inputs to logistic regression. This might give us a better hypothesis than just using the raw features would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and Intuitions I\n",
    "\n",
    "We can use neural networks to model logical operators, like $x_1 \\text{ AND } x_2$, $x_1 \\text{ OR  }x_2$, $x_1 \\text{ XOR } x_2$ and so on.\n",
    "\n",
    "For example, let's build a neural network that models the OR function.\n",
    "\n",
    "We set our first theta matrix as:\n",
    "\n",
    "$$ \\Theta^{(1)} = [-10 \\quad 20 \\quad 20] $$\n",
    "\n",
    "The choice of values is somewhat arbitrary - but $\\Theta_1 > \\Theta_0$, $\\Theta_2 > \\Theta_0$ and $\\Theta_1 + \\Theta_2 > \\Theta_0$. So when we come to work out $h_{\\Theta}(x) = g(\\Theta_0 + \\Theta_1x_1 + \\Theta_2x_2)$ we get the following truth table:\n",
    "\n",
    "![credit: Andrew Ng Coursera](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/f_ueJLGnEea3qApInhZCFg_a5ff8edc62c9a09900eae075e8502e34_Screenshot-2016-11-23-10.03.48.png?expiry=1490227200000&hmac=Addo3fvQpW1Icn5JzkSON5o-dUxitYiFBV3JknAsXIo)\n",
    "\n",
    "That's:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "   & x_1 = 0 \\\\\n",
    "   & x_2 = 0 \\\\\n",
    "   & g(-10 + 20 \\times 0 + 20 \\times 0) = g(-10) \\approx 0\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "   & x_1 = 0 \\\\\n",
    "   & x_2 = 1 \\\\\n",
    "   & g(-10 + 20 \\times 0 + 20 \\times 1) = g(10) \\approx 1\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "   & x_1 = 1 \\\\\n",
    "   & x_2 = 0 \\\\\n",
    "   & g(-10 + 20 \\times 1 + 20 \\times 0) = g(10) \\approx 1\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "   & x_1 = 1 \\\\\n",
    "   & x_2 = 1 \\\\\n",
    "   & g(-10 + 20 \\times 1 + 20 \\times 1) = g(30) \\approx 1\n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "Remember that $g(z)$ is the sigmoid function and anything $\\approx 4$ or more is $\\approx 1$, and anything $\\approx -4$ or less is $\\approx 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples and Intuitions II\n",
    "\n",
    "We saw above how to make a neural network that models the OR function. The theta matrix for OR is:\n",
    "\n",
    "$$ \\Theta^{(1)} = [-10 \\quad 20 \\quad 20] $$\n",
    "\n",
    "We can model AND and NOR with the following theta matrices:\n",
    "\n",
    "$$ \\text{AND: } \\Theta^{(1)} = [-30 \\quad 20 \\quad 20] $$\n",
    "$$ \\text{NOR: } \\Theta^{(1)} = [10 \\quad -20 \\quad -20] $$\n",
    "\n",
    "These can be combined to model XNOR (output is 1 if and only if $x_1$ and $x_2$ are the same).  \n",
    "\n",
    "We'll use a neural network with three layers. To map between the first and second layer, we'll use a theta matrix that combines the AND and NOR theta matrices:\n",
    "\n",
    "$$ \\Theta^{(1)} = \\begin{bmatrix}\n",
    "-30 \\quad \\quad 20 \\quad \\quad 20 \\\\\n",
    "\\quad 10 \\quad -20 \\quad -20 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "This is because AND outputs 1 iff $x_1$ and $x_2$ are 1 and NOR outputs 1 iff $x_1$ and $x_2$ are 0. XNOR outputs 1 iff $x_1$ and $x_2$ are the same, but it doesn't care if they're 1 or 0. So if the output of AND is 1 or the output of NOR is 1 then the output of XNOR is 1. So we can use the theta matrix for OR to map between the second and third layers.\n",
    "\n",
    "$$ \\Theta^{(2)} = [-10 \\quad 20 \\quad 20] $$\n",
    "\n",
    "![Andrew Ng Coursera](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/rag_zbGqEeaSmhJaoV5QvA_52c04a987dcb692da8979a2198f3d8d7_Screenshot-2016-11-23-10.28.41.png?expiry=1490486400000&hmac=pzIto1f-WOtBDz4-TmK_gjLjNdc4AaXAbNU2gIE4aS0)\n",
    "\n",
    "The values for our nodes are:\n",
    "$$ \\begin{align*}& a^{(2)} = g(\\Theta^{(1)} \\cdot x) \\newline& a^{(3)} = g(\\Theta^{(2)} \\cdot a^{(2)}) \\newline& h_\\Theta(x) = a^{(3)}\\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "The method used for multiclass classification is similar to the one-vs-all method for logistic regression. Instead of either 1 or zero, our output is a vector of 1s and 0s.\n",
    "\n",
    "![Andrew Ng Coursera](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/9Aeo6bGtEea4MxKdJPaTxA_4febc7ec9ac9dd0e4309bd1778171d36_Screenshot-2016-11-23-10.49.05.png?expiry=1490486400000&hmac=6gkrNE6LW7xAyFOoVW_J5IpxjZ6KdeOEbqCQLGpL30k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
