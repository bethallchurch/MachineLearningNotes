{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advice for Applying Machine Learning\n",
    "\n",
    "## Evaluating a Learning Algorithm\n",
    "\n",
    "### Deciding What to Try Next\n",
    "\n",
    "What should you do if your (e.g.) regularised linear regression model is making unacceptably large errors in its predictions?\n",
    "\n",
    "Options include:\n",
    "\n",
    "+ Collect more training examples.\n",
    "+ Try smaller sets of features.\n",
    "+ Try getting additional features.\n",
    "+ Try adding polynomial features.\n",
    "+ Try decreasing $\\lambda$.\n",
    "+ Try increasing $\\lambda$.\n",
    "\n",
    "How do you know which to try first?\n",
    "\n",
    "Machine learning diagnostic:  \n",
    "A diagnostic is a test that you can run to gain insight into what is or isn't working with a learning algorithm, and gain guidance as to how best to improve its performance.\n",
    "\n",
    "Diagnostics can take time to implement, but are worth it in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Hypothesis\n",
    "\n",
    "A really low value of training error might indicate overfitting rather than accuracy. For this reason you should always divide your (randomly ordered) data into a training set and a test set - a 70:30 split is typical - and check the error on the test set after the model has been trained with the training set data. A low value for training error and a high value for test error suggests overfitting.\n",
    "\n",
    "Procedure:  \n",
    "\n",
    "+ Learn parameter $\\theta$ from training data (minimising training error $J(\\theta)$).\n",
    "+ Compute test error (for e.g. linear regression): $$J_{test}(\\theta) = \\frac{1}{2m} \\sum^{m_{test}}_{i = 1} (h_\\theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$$\n",
    "+ Compare test error with training error.\n",
    "\n",
    "For logistic regression, you can use test error as follows:\n",
    "\n",
    "$$J_{test}(\\theta) = -\\frac{1}{m_{test}} \\sum^{m_{test}}_{i = 1} y^{(i)}_{test} \\log h_\\theta(x^{(i)}_{test}) + (1 - y^{(i)}_{test})\\log h_\\theta(x^{(i)}_{test}) $$\n",
    "\n",
    "Or an alternative error metric, the misclassification error:\n",
    "\n",
    "$$ err(h_\\theta(x), y) =\n",
    "\\begin{cases}\n",
    " 1 \\text{ if } h_{x} \\geq 0.5, y = 1 \\text{ or } h_{x} < 0.5, y = 0 \\\\\n",
    " 0 \\text{ otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "That is, 1 if the sample was misclassified, 0 otherwise. Then average these values to give you an idea of how many samples your hypothesis is misclassifying:\n",
    "\n",
    "$$ \\text{test error } = \\frac{1}{m_{test}} \\sum^{m_{test}}_{i = 1} err(h_\\theta(x^{(i)}_{test}), y^{(i)}_{test}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Selection and Train/Validation/Test Sets\n",
    "\n",
    "How do you decide what degree of polynomial to fit to a data set? Remember that fitting well to a training set doesn't tell you much about how well a model will generalise.\n",
    "\n",
    "Options:\n",
    "\n",
    "1\\. $h_\\theta(x) = \\theta_0 + \\theta_1x; d = 1$<br>\n",
    "2\\. $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2; d = 2$<br>\n",
    "3\\. $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3; d = 3 \\\\ \\vdots \\\\$<br>\n",
    "10\\. $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3 + \\dots + \\theta_{10}x^{10}; d = 10$<br>\n",
    "\n",
    "$d$ is the degree of polynomial, which you can think of as an extra parameter. If you choose which $d$ to use based on $J(\\theta)$ values for the test set, then the parameter $d$ will have been fitted to the test set and you can't tell if it will generalise.\n",
    "\n",
    "So, split your data into a training set, cross validation set, and test set (a typical ratio is 60:20:20). Then use the cross validation set to fit the parameter $d$ and check how well it generalises with the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bias vs. Variance\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Spam Classifier\n",
    "\n",
    "### Prioritizing What to Work On\n",
    "\n",
    "When building a spam classifier you need to decide how to represent the input features. You could look through a set of labelled training data and choose a list of the most common words present in spam and not-spam emails, and represent their presence or absence as 1 or 0 in a feature vector.  What would be the best way to spend your time to improve the accuracy of the classifier? Several options present themselves:\n",
    "\n",
    "+ Collect lots of data.\n",
    "+ Develop sophisticated features based on email routing information (from the email header).\n",
    "+ Develop sophisticated features for the message body, e.g. should 'deal' and 'dealer' be considered the same word?\n",
    "+ Develop sophisticated ways of processing your input data, like detecting deliberate misspellings designed by spammers to get around classifers, e.g. 'w4tches'.\n",
    "\n",
    "It's not obvious which of these is the most promising option.\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "Recommended approach:  \n",
    "\n",
    "+ Start with a simple quick and dirty algorithm that you can implement quickly (~24 hrs). Implement it and test it on your cross-validation data.\n",
    "+ Plot learning curves to decide if more data, more features, etc. are likely to help. This is a better approach than spending ages on the initial implementation, because at that point it's not clear what will be the most valuable use of your time (premature optimisation). So it's important to get something quickly and then base how you're going to spend your time on evidence.\n",
    "+ Error analysis: Manually examine the examples (in the cross validation set) that your algorithm misclassified. See if you can spot any systematic trend in what type of examples it is making errors on. For example, spam classifier might misclassify 100 emails, and when you look at them you can categorise them as:\n",
    "    - Pharma: 12\n",
    "    - Replica/fake: 4\n",
    "    - Phishing: 53\n",
    "    - Other: 31  \n",
    "So you know you need to think about what features could help your algorithm classify phishing emails correctly.\n",
    "\n",
    "It is very important to get error results as a single, numerical value (e.g. cross validation error). This is so you can compare the algorithm's performance under certain conditions with its performance under others. For example, you might not be sure whether or not stemming software would improve your algorithm's performance, so you need to try it with and without and compare the performance of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Skewed Data\n",
    "\n",
    "### Error Metrics for Skewed Classes\n",
    "\n",
    "Say we had an algorithm for predicting whether or not a person has cancer, and this algorithm has 1% cross validation error. However, only 0.5% of patients actually have cancer so on this metric an algorithm that predicted never predicted cancer would outperform it with only 0.5% error. In these sorts of cases where we have skewed classes such that the number of positive cases is much smaller than the number of negative cases, we need to turn to alternative error metrics to evaluate our algorithms.\n",
    "\n",
    "<table style=\"border:none;\">\n",
    "    <tr style=\"border:none;\">\n",
    "        <td style=\"border:none;\"></td>\n",
    "        <td style=\"border:none;\"></td>\n",
    "        <th colspan=\"2\" style=\"text-align:center;\">Actual class</th>\n",
    "    </tr>\n",
    "    <tr style=\"border:none;\">\n",
    "        <td style=\"border:none;\"></td>\n",
    "        <td style=\"border:none;\"></td>\n",
    "        <th style=\"text-align:center;\">1</th>\n",
    "        <th style=\"text-align:center;\">0</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th rowspan=\"2\" style=\"text-align:center;\">Predicted<br>class</th>\n",
    "        <th>1</th>\n",
    "        <td>True positive</td>\n",
    "        <td>False positive</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>0</th>\n",
    "        <td>False negative</td>\n",
    "        <td>True negative</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$y = 1$ in presence of rare class we want to detect.\n",
    "\n",
    "**Precision**  \n",
    "\"Of all patients where we predicted $y = 1$, what fraction actually has cancer?\"\n",
    "\n",
    "$$\n",
    "\\frac{\\text{# True positives}}{\\text{# Predicted positives}} = \\frac{\\text{# True positives}}{\\text{# True positives} + \\text{# False positives}}\n",
    "$$\n",
    "\n",
    "**Recall**  \n",
    "\"Of all patients that actually have cancer, what fraction did we correctly detect as having cancer?\"\n",
    "\n",
    "$$\n",
    "\\frac{\\text{# True positives}}{\\text{# Actual positives}} = \\frac{\\text{# True positives}}{\\text{# True positives} + \\text{# False negatives}}\n",
    "$$\n",
    "\n",
    "An algorithm that predicted never predicted cancer would have a recall of 0, so we can see that it's not a good classifier.\n",
    "\n",
    "### Trading Off Precision and Recall\n",
    "\n",
    "Say we had a logistic regression classifer such that:\n",
    "\n",
    "$$\n",
    "\\text{Predict } 1 \\text{ if } h_\\theta(x) \\geq 0.5\\\\\n",
    "\\text{Predict } 0 \\text{ if } h_\\theta(x) < 0.5\n",
    "$$\n",
    "\n",
    "But then we decided that we wanted to only predict a positive case if we were extra confident, so we changed the threshold:\n",
    "\n",
    "$$\n",
    "\\text{Predict } 1 \\text{ if } h_\\theta(x) \\geq 0.7\\\\\n",
    "\\text{Predict } 0 \\text{ if } h_\\theta(x) < 0.7\n",
    "$$\n",
    "\n",
    "This would result in higher precision and lower recall. Now say we wanted to do the opposite:\n",
    "\n",
    "$$\n",
    "\\text{Predict } 1 \\text{ if } h_\\theta(x) \\geq 0.3\\\\\n",
    "\\text{Predict } 0 \\text{ if } h_\\theta(x) < 0.3\n",
    "$$\n",
    "\n",
    "This would result in higher recall and lower precision.\n",
    "\n",
    "In general you can plot precision against recall as the threshold varies and the result will be a downwards curve (although its exact shape will depend on the details of the classifer).\n",
    "\n",
    "Is there a way to automatically choose the threshold to make sure you're using the best algorithm?\n",
    "\n",
    "The problem with using precision and recall as error metrics is that you no longer have a single numerical value you can use to compare algorithms. For example, it's not obvious which algorithm here performs the best:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <th>Precision (P)</th>\n",
    "        <th>Recall (R)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Algorithm 1</th>\n",
    "        <td>0.5</td>\n",
    "        <td>0.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Algorithm 2</th>\n",
    "        <td>0.7</td>\n",
    "        <td>0.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Algorithm 3</th>\n",
    "        <td>0.02</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "How can we combine precision and recall into a single useful value? We could try averaging them, but that won't work very well because, for example, if we have a classifier that predicts $y = 1$ all the time, then you can get a very high recall, but a very low value of precision.\n",
    "\n",
    "So instead we use the $F_1$ Score:\n",
    "\n",
    "$$\n",
    "F_1 \\text{ Score } = 2 \\frac{PR}{P + R}\n",
    "$$\n",
    "\n",
    "Which is similar to taking the average, but it gives the lower value a higher weight. In general:\n",
    "\n",
    "$$\n",
    "P = 0 \\text{ OR } R = 0 \\Rightarrow F_1 \\text{ Score } = 0 \\\\\n",
    "P = 1 \\text{ AND } R = 1 \\Rightarrow F_1 \\text{ Score } = 1\n",
    "$$\n",
    "\n",
    "If we calculate the $F_1$ Score for the algorithms:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <th>Precision (P)</th>\n",
    "        <th>Recall (R)</th>\n",
    "        <th>$F_1$ Score</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Algorithm 1</th>\n",
    "        <td>0.5</td>\n",
    "        <td>0.4</td>\n",
    "        <td>0.444</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Algorithm 2</th>\n",
    "        <td>0.7</td>\n",
    "        <td>0.1</td>\n",
    "        <td>0.175</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Algorithm 3</th>\n",
    "        <td>0.02</td>\n",
    "        <td>1.0</td>\n",
    "        <td>0.0392</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Then we can see that algorithm 1 is the one that we should use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Large Data Sets\n",
    "\n",
    "### Data for Machine Learning\n",
    "\n",
    "Under certain conditions the following saying holds true:\n",
    "\n",
    "> *It's not who has the best algorithm that wins. It's who has the most data.*\n",
    "\n",
    "What conditions are these?\n",
    "\n",
    "Assume feature $x \\in \\mathbb{R}^{n + 1}$ has sufficient information to predict $y$ accurately. (A useful test: Given the input $x$, can a human expert confidently predict $y$?)\n",
    "\n",
    "If you use a learning algorithm with many parameters (e.g. logistic regression / linear regression with many features; a neural network with many hidden units) then this algorithm will have low bias, and so $J_{\\text{train}}(\\theta)$ will be low.\n",
    "\n",
    "If you also have a large training set, then your algorithm will be unlikely to overfit, i.e. it will have low variance, and so $J_{\\text{train}}(\\theta) \\approx J_{\\text{test}}(\\theta)$.\n",
    "\n",
    "Combined, this should give you an effective algorithm will low bias and low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
