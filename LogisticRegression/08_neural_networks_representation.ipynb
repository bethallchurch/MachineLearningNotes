{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Neural Networks: Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations\n",
    "\n",
    "### Non-linear Hypotheses\n",
    "\n",
    "Say we had a classification task with two features and the 'line' separating the positive class from the negative was a fairly complicated shape, such that to represent this line we needed to include the quadratic and 3rd degree polynomial terms in the hypothesis.\n",
    "\n",
    "Our hypothesis would look something like:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2 + \\theta_5x_1x_2 + \\theta_6x_1^3 + \\theta_7x_2^3 + \\theta_8x_1^2x_2 + \\dots )$$\n",
    "\n",
    "Even with just two original features and including up to only the 3rd degree polynomial, the number of overall features has increased enormously.\n",
    "\n",
    "A lot of problems do require a lot of input features. For example, in computer vision if you use pixels as input features even if you restricted yourself to just grayscale 50x50 images you would have 2500 original input features. If you wanted to include the quadratic terms that gives you 3 million features, already too large to be reasonable.\n",
    "\n",
    "### Neurons and the Brain\n",
    "\n",
    "Neural networks are biologically inspired learning algorithms, i.e. algorithms that mimic the brain. Recent resurgence as it was only recently that computers became fast enough to run large scale neural networks.\n",
    "\n",
    "\"One learning algorithm\" hypothesis:  \n",
    "The brain uses a single learning algorithm. Evidence: successful neuro-rewiring experiments, e.g. rewiring auditory cortex so that it's being fed signals from the optic nerve; auditory cortex then learns to see. Idea is that if a single piece of brain tissue that can process sight / sound / touch then maybe there's one learning algorithm that can process sight / sound / touch. If we can figure out what that learning algorithm is and it we'll be sorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Model Representation I\n",
    "\n",
    "Neurons have dendrites, which can be thought of as \"input wires\", and an axon, which can be thought of as an \"output wire\". When a neuron receives a bunch of electrical pulses via its dendrites its axon will either fire an electrical pulse (to be received as an input to another neuron's dendrites) or not.\n",
    "\n",
    "We represent a single neuron as a \"logistic unit\":\n",
    "\n",
    "![logistic unit; credit: Andrew Ng Coursera](https://beths3test.s3.amazonaws.com/machine-learning-notes/neuron.png)\n",
    "\n",
    "In this diagram:\n",
    "\n",
    "$$ x = \\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\theta = \\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\theta_3 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "You can draw the $x_0$ node (the \"bias unit/node\"), but since it's always 0 it often gets left out.  \n",
    "As we're using a sigmoid function, this neuron is said to have a sigmoid (logistic) activation function.\n",
    "\n",
    "A neural network is a load of these logistic units connected together:\n",
    "\n",
    "![neural network; credit: Andrew Ng Coursera](https://beths3test.s3.amazonaws.com/machine-learning-notes/neural-network.png)\n",
    "\n",
    "In this diagram, layer 1 is the input layer, layer 3 is the output layer and layer 2 the hidden layer. Again, for each layer (except the output layer) there is a bias unit that isn't always drawn.\n",
    "\n",
    "$a^{(j)}_i$ is the \"activation\" of unit $i$ in layer $j$.  \n",
    "$\\Theta^{(j)}$ is the matrix of weights controlling function mapping from layer $j$ to layer $j + 1$.\n",
    "\n",
    "$h_\\Theta(x)$ in this network gets computed the following way:\n",
    "\n",
    "First, the activations of the nodes in the hidden layer are calculated:\n",
    "\n",
    "$$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2) + \\Theta_{13}^{(1)}x_3) \\\\\n",
    "a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2) + \\Theta_{23}^{(1)}x_3) \\\\\n",
    "a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2) + \\Theta_{33}^{(1)}x_3)$$\n",
    "\n",
    "Then, these results are used to calculate the output:\n",
    "\n",
    "$$ h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)}) + \\Theta_{13}^{(2)}a_3^{(2)}) $$\n",
    "\n",
    "If a network has $s_j$ units in layer $j$, $s_{j + 1}$ units in layer $j + 1$, then $\\Theta^{(j)}$ will be of dimension $s_{j + 1} \\times (s_j + 1)$.\n",
    "\n",
    "In our diagram, there are 3 units in layer 1 and 3 units in layer 2, and so $\\Theta^{(1)}$ is a $(3\\times4)$ matrix. There are 3 units in layer 2 and 1 in layer 3, so $\\Theta^{(2)}$ is a $(1\\times4)$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation II\n",
    "\n",
    "Let's try and work out a vectorized implementation of the neural network described above.\n",
    "\n",
    "Before we described our network as:\n",
    "\n",
    "$$ a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2) + \\Theta_{13}^{(1)}x_3) \\\\\n",
    "a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2) + \\Theta_{23}^{(1)}x_3) \\\\\n",
    "a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2) + \\Theta_{33}^{(1)}x_3) \\\\\n",
    "h_\\Theta(x) = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)}) + \\Theta_{13}^{(2)}a_3^{(2)}) $$\n",
    "\n",
    "Let's write the parameters passed to $g(z)$ like this:\n",
    "\n",
    "$$ z_1^{(2)} = \\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2) + \\Theta_{13}^{(1)}x_3 \\\\\n",
    "z_2^{(2)} = \\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2) + \\Theta_{23}^{(1)}x_3 \\\\\n",
    "z_3^{(2)} = \\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2) + \\Theta_{33}^{(1)}x_3 $$\n",
    "\n",
    "So now we can say that:\n",
    "\n",
    "$$ a_1^{(2)} = g(z_1^{(2)}) \\\\\n",
    "a_2^{(2)} = g(z_2^{(2)}) \\\\\n",
    "a_3^{(2)} = g(z_3^{(2)})$$\n",
    "\n",
    "Let's define the vector $z^{(2)}$ as:\n",
    "\n",
    "$$ z^{(2)} = \\begin{bmatrix}\n",
    "z_1^{(2)} \\\\\n",
    "z_2^{(2)} \\\\\n",
    "z_3^{(2)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you look at how the element's of $z^{(2)}$ are defined you can see that $z^{(2)} = \\Theta^{(1)}x$. So we can calculate the vector $a^{(2)}$ (the second, hidden layer) in just two steps:\n",
    "\n",
    "$$ z^{(2)} = \\Theta^{(1)}x \\\\\n",
    "   a^{(2)} = g(z^{(2)})$$\n",
    "   \n",
    "Now we need to add the bias unit $a_0^{(2)} (= 1)$ to the hidden layer so that:\n",
    "\n",
    "$$ a^{(2)} = \\begin{bmatrix}\n",
    "a_0^{(2)} \\\\\n",
    "a_1^{(2)} \\\\\n",
    "a_2^{(2)} \\\\\n",
    "a_3^{(2)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And now we can write:\n",
    "\n",
    "$$z^{(3)} = \\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)}) + \\Theta_{13}^{(2)}a_3^{(2)}$$\n",
    "\n",
    "As:\n",
    "\n",
    "$$ z^{(3)} = \\Theta^{(2)}a^{(2)} $$\n",
    "\n",
    "So that the final output of the network is:\n",
    "\n",
    "$$ h_\\Theta(x) = a^{(3)} = g(z^{(3)}) $$\n",
    "\n",
    "This process is known as *forward propagation* because each layer is computed and the result used to compute the next layer until we arrive at the output.\n",
    "\n",
    "This is cool because instead of being constrained to use the raw features $x$, this network gets to learn the features $a^{(2)}$, and use those as inputs to logistic regression. This might give us a better hypothesis than just using the raw features would."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
