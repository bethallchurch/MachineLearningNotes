{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Overfitting\n",
    "\n",
    "Underfitting: If we have too few features, the form of our learned hypothesis may map poorly to the trend of the data, even in the training set.\n",
    "\n",
    "Overfitting: If we have too many features, the learned hypothesis may fit the training set very well but fail to generalize to new examples.\n",
    "\n",
    "![Underfitting, just right, overfitting](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/0cOOdKsMEeaCrQqTpeD5ng_2a806eb8d988461f716f4799915ab779_Screenshot-2016-11-15-00.23.30.png?expiry=1489881600000&hmac=Ia8gpN6pJpLinzNrjDMHbothgCmyoKTSxvqpwOKPfEI)\n",
    "\n",
    "This can be a problem in both linear and logistic regression.\n",
    "\n",
    "The main options to address it are:\n",
    "\n",
    "1. Reduce the number of features:\n",
    "    - Manually select which features to keep.\n",
    "    - Use a model selection algorithm.\n",
    "2. Regularization:\n",
    "    - Keep all the features, but reduce the magnitude of parameters $\\theta_j$.\n",
    "    - Regularization works well when we have a lot of slightly useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "To reduce the magnitude of parameters $\\theta$ we can slightly modify the cost function. For example, to reduce the magnitude of $\\theta_3x^3$ and $\\theta_4x^4$ we can adapt the cost function as follows:\n",
    "\n",
    "$$ min_\\theta\\ \\dfrac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + 1000\\cdot\\theta_3^2 + 1000\\cdot\\theta_4^2 $$\n",
    "\n",
    "To compensate for the 1000s, $\\theta_3$ and $\\theta_4$ will be given very low values, reducing the complexity of the form of the hypothesis.\n",
    "\n",
    "If we don't know which parameters to regularize, we could regularize them all by modifying the cost function like this:\n",
    "\n",
    "$$ min_\\theta\\ \\dfrac{1}{2m}\\ \\left[ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\ \\sum_{j=1}^n \\theta_j^2 \\right] $$\n",
    "\n",
    "$\\lambda$ is known as the regularization parameter. It determines how much the costs of our theta parameters are inflated. Used correctly it can help smooth out the curve to avoid overfitting. However, if $\\lambda$ is too big it might smooth out the curve too much so that all the parameters are approximately 0, resulting in underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression\n",
    "\n",
    "What does gradient descent look like with our modified cost function?\n",
    "\n",
    "$$ \\begin{align*} & \\text{Repeat}\\ \\lbrace \\newline & \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline & \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2,\\dots,n\\rbrace\\newline & \\rbrace \\end{align*} $$\n",
    "\n",
    "Note that we separate out $\\theta_0$ because we don't want to penalize it.\n",
    "\n",
    "The update rule can be rewritten as:\n",
    "\n",
    "$$ \\theta_j := \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $$\n",
    "\n",
    "$(1 - \\alpha\\frac{\\lambda}{m})$ is always a number less than 1 (and greater than 0 I think). $- \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ was our rule before regularization, so all we're doing is the same thing but with the additional step of multiplying $\\theta_j$ by some number less than 1 to scale it down.\n",
    "\n",
    "What does the normal equation look like with our modified cost function?\n",
    "\n",
    "$$ \\begin{align*}& \\theta = \\left( X^TX + \\lambda \\cdot L \\right)^{-1} X^Ty \\newline& \\text{where}\\ \\ L = \\begin{bmatrix} 0 & & & & \\newline & 1 & & & \\newline & & 1 & & \\newline & & & \\ddots & \\newline & & & & 1 \\newline\\end{bmatrix}\\end{align*} $$\n",
    "\n",
    "L is a matrix with 0 at the top left and 1's down the diagonal, with 0's everywhere else. It should have dimension $(n+1)Ã—(n+1)$. Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number $\\lambda$.\n",
    "\n",
    "Recall that if $m \\leq n$, then $X^TX$ is non-invertible. However, when we add the term $\\lambda \\cdot L$, then $X^TX$ + $\\lambda \\cdot L$ becomes invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "What does gradient descent for logistic regression look like with our modified cost function? Superficially, it actually looks exactly the same as for linear regression, i.e.:\n",
    "\n",
    "$$ \\begin{align*} & \\text{Repeat}\\ \\lbrace \\newline & \\ \\ \\ \\ \\theta_0 := \\theta_0 - \\alpha\\ \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\newline & \\ \\ \\ \\ \\theta_j := \\theta_j - \\alpha\\ \\left[ \\left( \\frac{1}{m}\\ \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j \\right] &\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ j \\in \\lbrace 1,2,\\dots,n\\rbrace\\newline & \\rbrace \\end{align*} $$\n",
    "\n",
    "Although obviously it's not exactly the same because $h_\\theta(x^{(i)})$ is different in logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
