{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "## Large Margin Classification\n",
    "\n",
    "### Optimization Objective\n",
    "\n",
    "Support vector machine is another supervised learning algorithm. We can get the support vector machine cost function by modifying the cost function for logistic regression.\n",
    "\n",
    "Our hypothesis for logistic regression is:\n",
    "\n",
    "$$ h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}} $$\n",
    "\n",
    "If $y = 1$, then we want $h_\\theta(x) \\approx 1$, and so we want $\\theta^Tx$ to be much greater than 0. And if $y = 0$, then we want $h_\\theta(x) \\approx 0$, and $\\theta^Tx$ to be much less than 0.\n",
    "\n",
    "Each training example contributes a term like this to the overall cost function:\n",
    "\n",
    "$$-y\\log(\\frac{1}{1 + e^{-\\theta^Tx}}) - (1 - y)\\log(1 - \\frac{1}{1 + e^{-\\theta^Tx}})$$\n",
    "\n",
    "Consider the case where $y = 1$. In this case, only the term $-\\log(\\frac{1}{1 + e^{-\\theta^Tx}})$ from the cost function above matters, as the other is cancelled out.\n",
    "\n",
    "To build a support vector machine we're going to modify the $-\\log(\\frac{1}{1 + e^{-\\theta^Tx}})$ cost function a little bit.\n",
    "\n",
    "In the case of $y = 0$, we need to modify the term $\\log(1 - \\frac{1}{1 + e^{-\\theta^Tx}})$. \n",
    "\n",
    "Let's say we're going to replace $-\\log(\\frac{1}{1 + e^{-\\theta^Tx}})$ with $\\text{cost}_1(\\theta^Tx)$ and $-\\log(1 - \\frac{1}{1 + e^{-\\theta^Tx}})$ with $\\text{cost}_0(\\theta^Tx)$.\n",
    "\n",
    "The overall cost function for logistic regression is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\bigg[\\sum_{i=1}^{m} y^{(i)}-\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})-\\log(1 - h_\\theta(x^{(i)}))\\bigg] + \\frac{\\lambda}{2m}\\sum^n_{j=1}\\theta^2_j $$\n",
    "\n",
    "And the overall cost function for support vector machine is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\bigg[\\sum_{i=1}^{m} y^{(i)}\\text{cost}_1(\\theta^Tx) + (1 - y^{(i)})\\text{cost}_0(\\theta^Tx)\\bigg] + \\frac{\\lambda}{2m}\\sum^n_{j=1}\\theta^2_j $$\n",
    "\n",
    "By convention, we don't use the $\\frac{1}{m}$ terms in support vector machines, and we let $C = \\frac{1}{\\lambda}$, so the cost function is more usually written as:\n",
    "\n",
    "$$ J(\\theta) = C \\bigg[\\sum_{i=1}^{m} y^{(i)}\\text{cost}_1(\\theta^Tx) + (1 - y^{(i)})\\text{cost}_0(\\theta^Tx)\\bigg] + \\frac{1}{2}\\sum^n_{j=1}\\theta^2_j $$\n",
    "\n",
    "But these two ways of expressing the cost function are equivalent.\n",
    "\n",
    "The hypothesis of the support vector machine doesn't output a probability, just a prediction 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Large Margin Intuition\n",
    "\n",
    "Large margin classifiers - what does that mean?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
